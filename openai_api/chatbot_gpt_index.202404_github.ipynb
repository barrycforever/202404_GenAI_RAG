{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c546dcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c546dcd",
    "outputId": "3789bdd5-3464-4ccd-c4dc-deccb3b60a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.10.28-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
      "  Downloading llama_index_agent_openai-0.2.2-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Downloading llama_index_cli-0.1.11-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.28 (from llama_index)\n",
      "  Downloading llama_index_core-0.10.28-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.7-py3-none-any.whl.metadata (603 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.1.15-py3-none-any.whl.metadata (559 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
      "  Downloading llama_index_program_openai-0.1.5-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.1.16-py3-none-any.whl.metadata (934 bytes)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
      "  Downloading openai-1.16.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.28->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (3.9.3)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (2023.10.0)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.16 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading llamaindex_py_client-0.1.16-py3-none-any.whl.metadata (711 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (8.2.2)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (4.9.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.28->llama_index) (1.14.1)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Downloading PyMuPDF-1.24.1-cp311-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index)\n",
      "  Downloading llama_parse-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama_index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/anaconda3/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.16->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.10.12)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama_index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama_index) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama_index) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama_index) (2023.10.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.8.0)\n",
      "Collecting PyMuPDFb==1.24.1 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
      "  Downloading PyMuPDFb-1.24.1-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.28->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.28->llama_index) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.28->llama_index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.0.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.28->llama_index)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama_index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama_index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.28->llama_index) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.28->llama_index) (1.16.0)\n",
      "Downloading llama_index-0.10.28-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_agent_openai-0.2.2-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.11-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_core-0.10.28-py3-none-any.whl (15.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m941.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m958.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.15-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.5-py3-none-any.whl (4.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.16-py3-none-any.whl (36 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_parse-0.4.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading llamaindex_py_client-0.1.16-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m967.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDF-1.24.1-cp311-none-macosx_11_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m747.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.1-py3-none-macosx_11_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m896.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m950.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, typing-inspect, tqdm, pypdf, PyMuPDFb, marshmallow, h11, deprecated, beautifulsoup4, tiktoken, pymupdf, httpcore, dataclasses-json, httpx, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "Successfully installed PyMuPDFb-1.24.1 beautifulsoup4-4.12.3 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-agent-openai-0.2.2 llama-index-cli-0.1.11 llama-index-core-0.10.28 llama-index-embeddings-openai-0.1.7 llama-index-indices-managed-llama-cloud-0.1.5 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.15 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.5 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.16 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.0 llama_index-0.10.28 llamaindex-py-client-0.1.16 marshmallow-3.21.1 openai-1.16.2 pymupdf-1.24.1 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.6.0 tqdm-4.66.2 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f98d647",
   "metadata": {
    "id": "4f98d647"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/gitconnected/creating-your-own-ai-powered-second-brain-a-guide-with-python-and-chatgpt-f5547ef7e136\n",
    "\n",
    "from llama_index.core import GPTVectorStoreIndex, SimpleDirectoryReader, Document\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jUCw8iiqyOdC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUCw8iiqyOdC",
    "outputId": "3423ce13-7459-4a74-fc14-68d151c115e9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ajO1L65nyVo_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajO1L65nyVo_",
    "outputId": "2ff16af0-23c4-4c84-9163-4c39e753256f"
   },
   "outputs": [],
   "source": [
    "# ! ls -l /content/gdrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2e09e6",
   "metadata": {
    "id": "5a2e09e6"
   },
   "outputs": [],
   "source": [
    "#downloads=\"/content/gdrive/MyDrive\"\n",
    "#dir=downloads+'/nu_benefits'              # Google Colab\n",
    "#downloads=\"c:/users/username/Downloads\"   # Windows\n",
    "downloads=\"/Users/user/Downloads\"          # Mac\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"PASTE-YOUR-OPENAI-API-KEY-HERE\"\n",
    "\n",
    "## Load data from the journal text file\n",
    "#documents = SimpleDirectoryReader(dir).load_data()\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = pd.read_excel(downloads+'/nu_benefits.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "from_xlsx = df['Text'].tolist()\n",
    "for i in range(len(from_xlsx)):\n",
    "    from_xlsx[i] = from_xlsx[i].replace(\"\\n\", \" \")\n",
    "    \n",
    "documents = [Document(text=t) for t in from_xlsx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5068f19e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5068f19e",
    "outputId": "14e37c74-83c5-4b03-c753-bc04a2636407"
   },
   "outputs": [],
   "source": [
    "# Create a simple vector index\n",
    "#index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "#index.save_to_disk(\"generated_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "855df817-6950-4a9a-a441-82d637263594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you want to quite, just press ENTER with no input.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  What PPOs are available?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Premier PPO, Select PPO, and Value PPO are the preferred provider options (PPOs) available in the medical plans offered by Northwestern.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "print (\"When you want to quite, just press ENTER with no input.\")\n",
    "# Create an infinite loop asking for user input and then breaking out of the loop when the response is empty\n",
    "while True:\n",
    "  query = input(\"Ask a question: \")\n",
    "  if not query:\n",
    "    print(\"Goodbye\")\n",
    "    break\n",
    "\n",
    "  # query the index with the question and print the result\n",
    "  result = query_engine.query(query)\n",
    "  print()\n",
    "  print(result)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd48839",
   "metadata": {
    "id": "afd48839"
   },
   "outputs": [],
   "source": [
    "# https://pypi.org/project/gpt-index/\n",
    "# https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html\n",
    "# https://towardsdev.com/part-1-create-a-chatgpt-for-you-own-documents-using-python-38224f162631\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401f695-b379-4efb-b94f-08c7b043bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the employee reduced tuition benefit?\n",
    "\n",
    "# What is reduced tuition benefit for my spouse for undergraduate classes?\n",
    "\n",
    "# What PPOs are available?\n",
    "\n",
    "# When am I eligible for benefits?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
